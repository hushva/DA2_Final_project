---
title: "Does a film's rating depend of the choice of genre?"
author: "Karola Tak√°cs"
date: "02/01/2021"
output:
  pdf_document: default
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
  html_document:
    default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Packages
# rm(list=ls())
library(WDI)
library(tidyverse)
require(scales)
library(lspline)
library(estimatr)
library(texreg)
library(ggthemes)
library(magrittr)
library(moments)
library(sjPlot)
library(xfun)
library(wesanderson)
# import clean csv from github repo
my_url_git <- 'https://raw.githubusercontent.com/hushva/DA2_Final_project/main/data/'
movies <- read_csv(paste0( my_url_git , 'clean/movies.csv' ) )
```

## 1. Executive Summary

I was interested in the connection of weighted average IMDB ratings and the genre of movies: is there a relationship indicating that some genres are more likely to be upvoted than others? The IMDB dataset is historically extensive however might be problematic since the same user base voted for old and for new films. Nonetheless the results are based on data from 1920 till 1995 on films which have more than 25 thousand votes to retain credibility on such subjective matter as taste. The analysis suggests that topics of Family, Horror or Fantasy tend to have lower ratings so screenwriters choose more 'likeable' themes or a director might decide for a book to adapt on screen which is not in these categories. I have also found that metacritic is strongly correlated with IMDB ratings but has not much causal relationship with it.

## 2. Introduction

The primary interest of this analysis was whether there is a causal connection between the imdb rating of a film and it's genre. I was simply interested if certain genres are more 'likeable' by viewers or more successful from a director's point of view. My original (broad) research question was:  Is there a favorable genre to achieve higher movie rating? And also What other factors might influence the rating? However after the data cleaning part, I managed to narrow down the research question to: Does the genre choice of a director yield to certain imdb ratings for movies older than 1920? (more precisely: for movies with more than 25000 reviews). With this information at a director's hands it would be a direction what are the seemingly "more successful" genres. 

## 3. Data

The main variables of the joined dataset to be analyzed are the movies' title, year of making, their genre along with ratings from two different sources: imdb and Metacritic (Metacritic aggregates movie reviews from the leading critics) and also number of reviews by imdb users and critics and finally the worldwide gross income in millions. The left-hand sided variable is rating and the right-hand side variable is genre.

To begin with I had two tables about movies both downloaded from the Kaggle webpage. They differed in record count, but after joining them based on the imdb movie ID field, I had 43605 observations. The year variable ranges from 1874 to 1995.  
During the cleaning process I excluded all the observations with missing values for the most important variables, imdb rating and genre, but these accounted only for 2031 observations. Let me explain these two variables in more detail.

* IMDB rating: all registered members of IMDB can cast their votes (good quality). For one movie a user can vote several times but then the last value is going to be updated, so basically there is 1 user for 1 film at a single time.

* Genre: this is a problematic variable since it not only is categorical but for a single movie there can be more genres attached. For simplification my assumption was: the firstly listed genre is the most relevant or primary genre. Even if this is true, it really makes sense to have more categories, because there are few main categories, like action or drama and it is easy to put most of the movies into one of these which results in that these categories will be overrepresented without other genres to balance out.

**Descriptive statistics** show that IMDB rating values are close to normal distribution, bit skewed to the left, signaling that he mean is well above 5, it is around 7. Similar with metacritic score, which has a better looking bell-shape, the distribution of values are wider, values are more spread on the scale of 100. IMDB Votes would make sense to transform for the analysis part as it shows right skewness, but since it is already incorporated into the IMDB rating variable it would be a bad conditioning variable in the visual analysis, so I decided not to use it. 


```{r distribution, echo=FALSE, warning = FALSE, out.width = '50%', fig.height=4}

movies %>% ggplot() +
  geom_histogram(aes(x = rating), bins = 45, fill = "darkorange", col="darkorange", alpha=0.7) +
  labs(title="Distribution of IMDB rating", x = "Weighted average of movie ratings") +
  scale_x_continuous(limits=c(1,10)) +
  ylab("") +
  theme_minimal()

genre_dist <- movies %>% group_by(genre) %>% 
  mutate(count_name = n())

ggplot(genre_dist,aes(x = reorder(genre, count_name, y = count_name))) +
    geom_bar(fill = "darkorange", color = "darkorange",stat = "count",alpha = .7, width = 0.7 ) +
    coord_flip() +
    labs(title="Number of various genres", x = "Genre") +
    ylab("") +
    theme_minimal()


```

```{r summary stats, echo=FALSE, warning = FALSE, out.width = '50%', fig.height=4}
ratings_stat <- movies %>% 
  summarise(
    Variable  = "IMDB ratings",
    mean     = round(mean(rating), 2),
    median   = round(median(rating),2),
    std      = round(sd(rating), 2),
    min      = round(min(rating),2),
    max      = round(max(rating),2),
    skew     = round(moments::skewness(rating), 2),
    numObs   = sum( !is.na( rating ) ) )
knitr::kable(ratings_stat,caption="Summary stat of IMDB ratings")
```

The distributions (see in Appendix - Histograms) of votes, gross income and user review suggest possible log transformations. After the transformations we can see an approximately normal distribution for gross income and number of reviews, however number of votes is still not close to normal distribution. In this case it is not a concern since I argued earlier why I am not intended to use this variable.

There are some **extreme values** for user_review, namely 8232, 6938 and 5392. But these are for films in the top 15, so it is possible, they might have a huge fan base and users left that many reviews. There is no need to drop these values.
Gross income is strongly left skewed and there will be a need for log transformation in the modelling part.

The number of records drop significantly after I decided to filter based on the number of votes. The baseline was 25 thousand number of votes and my rationale behind it was the fact that when creating the top 250 list, IMDB considers only movies with minimum this vote number. I think this makes the rating more credible so I decided to apply the same. In the very end I had 3624 observations to work with.

When thinking about **representativeness** I would say that IMDB ratings are only reliable for the IMDB users who rated the movie, but not necessarily reliable indicators of what general movie audiences thought; it depends a lot on personal taste and preferences. Old movies are not that much represented since I would argue lot less registered users have seen those (even less if we consider the whole population). I also thought of the geographical coverage: only those can vote who have internet access (can register on the IMDB page) and also language might be an issue. We also cannot be 100% sure that all voters have seen the film and their vote is the true assessment of their liking. All in all this dataset captures information for registered users' preferences and the findings would indicate the favorite genre of these users only.


## 4. Model
### 4.1 Setup

As mentioned earlier, the outcome variable is rating, the weighted (on number of votes) average user rating on IMDB and the parameter of interest is genre, for which I created a new column - their number of occurrence in the dataset - for possible weighting.
Before checking for potential confounder in the existing dataset, I noted several other factors which possibly can influence my outcome variable:

* availability of dubbing

* screening in how many countries (how worldwide the screening was) - this might relates to worldwide gross income

* lot harder to acquire and watch old films since they are not digitized or simply rare

* motion picture content rating system (whether a movie is rated only above 18 years of audience)

* leading actor/actress in the movie is famous/well known

Since I want to regress genres on ratings as the base idea, after checking these with lowess smoother (and after transforming genre into a factor), I could detect a few things:

* the most common genres are Action, Comedy, Adventure, Drama and Horror.

* Their ratings move on a wide range, while for instance Western has an average rating only above 7.5, similarly with Family or Film-Noir.

```{r genre plot, echo=FALSE, warning = FALSE, fig.align='center',out.width = '100%', fig.height=4}
movies$genre = as.factor(movies$genre)
# movies$genre <- factor(df$genre, levels = movies$genre)

ggplot( movies , aes(x = forcats::fct_rev(reorder(genre,genre)), y = rating), col = green ) +
  geom_point() +
  geom_smooth(method="loess" , formula = y ~ x )+
  coord_flip() +
  labs(title = "Lowess estimator for average movie rating and genre", y = "Weighted average of imdb ratings") +
  xlab("") +
  theme_minimal()
```

I wanted to check for a potential confounder effect between metacritic score and number of critics review since metacritic score is also based on critics reviews and scores, but there seems no pattern between the two.
```{r, echo=FALSE, warning = FALSE,fig.align='center', out.width = '50%', fig.height=4}
ggplot( movies , aes(x = critics_review, y = metacritic)) +
  geom_point() +
  geom_smooth(method="loess" , formula = y ~ x )+
  labs(title = "Lowess estimator for number of critics reviews and metacritic rating", x = "Number of critics review", y = "Metacritic ratings") +
  theme_minimal()

```
In this phase I also checked correlations among my X variables.The highest correlation (where correlation is above 0.7) is between user_review and votes. This is not surprising since I filtered for observations only with at least 25 thousand votes and these movies tend to have more user reviews. ucratio is also obviously highly correlated with user_review since it is a calculated variable based on user - and critics reviews. For this reason it is enough to use either the ratio or the two other variables but not both. The moderately correlated pairs are number of votes - gross income and votes - ucratio (I would guess since it incorporates user rating which is highly correlated with ucratio). 
```{r correlation, echo=FALSE, warning = FALSE, message = FALSE,fig.align='center', out.width = '50%', fig.height=4}
numeric_df <- keep( genre_dist , is.numeric )
cT <- cor(numeric_df , use = "complete.obs")

# Check for highly correlated values:
sum( cT >= 0.5 & cT != 1 ) / 2

# Find the correlations which are higher than 0.8
id_cr <- which( cT >= 0.5 & cT != 1 )
pair_names <- expand.grid( variable.names(numeric_df) , variable.names(numeric_df) )

# Get the pairs:
high_corr <- pair_names[ id_cr , ]
high_corr <- mutate( high_corr , corr_val = cT[ id_cr ] )

knitr::kable(high_corr, caption = "Mid - Highly correlated variable pairs", "simple")
```

For an example on interaction analysis see Appendix.

### 4.2 Modelling

The simple linear regression (1) was on ratings on the genre variable:

Rating E = $\beta_{0}$ + $\beta_{1}$genre.

At first glance on the model output, there are two categories - Horror and Fantasy - which have negative coefficients meaning these genres affect the ratings adversely. The reference category for the genre dummy is the Action category which has the highest record count in the data - **good reference category** - : the slopes doesn't seem significant, indicating no significant difference in the conditional means between the Action and other categories. For instance movies in the Adventure category received 0.4 higher rating on average than those in the Action category. I don't intend to go into more details with this model's interpretation, since its R square is really low: 0.10, similarly to models (2) and (3). So starting out with genre on the right hand side, is not very convincing. Adding duration and gross income to the model did not increase the model fit very much and the coefficients changed slightly.
Hence my choice of model is the (5):

Rating E = $\beta_{0}$+$\beta_{1}$genre+$\beta_{2}$$\ln$(duration)+$\beta_{3}$$\ln$(gross_income)+$\beta_{4}$metacritic+$\beta_{5}$$\ln$(ucratio)

More precicely:
Rating E = 5.26 + 0.03*genreAdvanture*_ + 0.39*genreAnimation*_ + 0.20*genreBiography* + 0.04*genreComedy* + 0.25*genreCrime* + 0.17*genreDrama* - 0.01*genreFamily* - 0.12*genreFantasy* - 0.13*genreHorror* + 0.09*genreMusical* + 0.29*genreMistery* + 0.52*genreRomance* + 0.16*genreSci-Fi* + 0.23*genreThriller* + 0.51*genreWestern* + 1.06*ln(duration)* - 0.04*ln(gross_income)* + 0.03*meatcritic* + 0.15*ln(ucratio)*


For this model the R square jumped to 0.61, the model is accounting for 61% of the variance in the data. On the other hand the number of observations are fewer with around 800, so the size of the data is not the same as for the other previous models which performed worse. Another issue I noted is that the coefficients are quite off compared to where they were earlier: differences are around between -0.7 and +1.3.
The output results suggest that on average, holding all other variables constant, a movie in category Adventure will have a higher rating wit 0.03 on average, than an Action movie, so basically almost the same. Biggest differences are with the Animation, Romance and Western categories: these genres are on average receiving higher ratings by 0.39, 0.59 and 0.51 respectively. The adverse effect of Fantasy and Horror remained but Family also turned out to have a negative coefficient.
When comparing films with the same length, increasing the duration by 10 percent, we expect the rating to go up by 0.106 percent, holding all else constant. Interestingly, it seems that a 10 percent higher USD worldwide gross income we expect the rating to decrease by 0.004, but that is very low

For possible robustness check first I would focus only on movies which are categorized in one of the top 5 categories: Action, Comedy, Drama, Crime and Adventure. Another option could be of filtering for only the highly rated (let's say above 7) movies, also would try a specific decade when the films were produced (e.g. the 80s). 


## 5. Generalization, external validity and causality

To start with, the explanatory variable *genre* is not accounting a lot for IMDB ratings alone and the model is difficult to interpret. Further control variables proved useful in explaining variation in the data, but not very convincing. It turned out that metacritic is the highest correlating variable (0.74) with rating, and when added to the model R squared increased by 0.35. If used instead of *genre* the model fit was 0.55. It is indeed an important variable, but I am not convinced that there is not a confounding variable involved - one potential variable could be *critics review* (critics review and worldwide_gross_income have a correlation of 0.4). I have listed previously some other factors I could think of influencing the movie ratings but they are hard to handle, too because they are categorical. I think the biggest con of this model is that my *genre* variable does not fully capture the information on this category. Based on this, one cannot expect to give good results if the set up is not comprehensive. The original dataset mostly contained qualitative data which also does not favor an easy model building. There was a potential right hand side variable *budget* that would make sense to incorporate: the hypothesis could be that the more money is invested in producing the film the higher it's quality and thus rating would be. Unfortunately it contained various currencies that would need to be converted to USD (I decided not to filter on observations with the USD currency because I believe that would make the analysis biased). One confounder I think could play an important role in a future analysis is whether a famous actor is playing the leading role - again, how to create the dummy can cause "headaches" (who is famous enough to be in the 'famous' category).
I also believe that I have used a wide selection of data in terms of *year*, but I could not say I have a strong internal validity, since older films were also evaluated by registered users who did watch that particular film in this decade and we cannot know how a person living in the 20s would evaluate a film made around that time or a newer film.
An other factor is, that ratings are highly personal and subjective and it is almost impossible to get all the angles on this matter but I would try to find solid quantitative factors indicative of these various opinions and build a model with those. From the results we can only generalize for the registered users of the IMDB webpage which is not necessarily the "single truth" for signaling how good a movie is.


## 6. Summary

I have analyzed the relationship between average IMDB movie rating and genres. This was a simple level-level regression, which  was then extended with the log transformed movie length, worldwide gross income and user-critics ratio (number of user reviews divided by number of critics reviews) and metacritic score without transformation. With the help of the model now I know how various genres compared to the base category is associated with higher or lower ratings. I think it is informative for a producer or writer to know that topics of Family, Horror or Fantasy tend to have lower ratings (in this sample), while for instance the highest difference is for Romance: on average, holding all other variables constant, a movie in this category will have a higher rating by 0.52 on average, than an Action movie. The second highest category turned out to be Western. These might signal that dispite there are very few observations with this primary genre category, these films can overtake the most common categories. The second finding was that though *metacritic* is strongly correlated with movie ratings, it doe not have a strong effect on it: changing metacritic score by one, we would expect IMDB rating to change by +0.03, on average, holding all other factors constant. So could it be that users registered on IMDB are also critics who post for metacritic? Certainly.


# Appendices

### Histograms
```{r, echo=FALSE, warning = FALSE, out.width = '50%', fig.height=4}

# duration histogram
movies %>% ggplot() +
  geom_histogram(aes(x = duration), bins = 40, fill = "darkorange", col="darkorange", alpha=.7) +
 labs(title="Distribution of movie length", x = "Movie duration (min)") +
  ylab("") +
   scale_x_continuous(limits=c(60,330)) +
theme_minimal()

# metacritic histogram
movies %>% ggplot() +
  geom_histogram(aes(x = metacritic), bins = 30, fill = "darkorange", col="darkorange", alpha= .7) +
  labs(title="Distribution of metacritic scores", x = "Metacritic score") +
  ylab("") +
  scale_x_continuous(limits=c(1,100)) +
  theme_minimal()

# votes histogram
movies %>% ggplot() +
  geom_histogram(aes(x = votes), bins = 45, fill = "darkorange", col="darkorange", alpha= .7) +
  labs(title="Distribution of number of votes", x = "Votes") +
  ylab("") +
  theme_minimal()

# gross income histogram
movies %>% ggplot() +
  geom_histogram(aes(x = worldwide_gross_income), bins = 50, fill = "darkorange", col="darkorange", alpha= .7) +
  labs(title="Distribution of movie gross income", x = "Gross income in millions (USD)") +
  ylab("") +
  theme_minimal()

# users review histogram
movies %>% ggplot() +
  geom_histogram(aes(x = user_review), bins = 50, fill = "darkorange", col="darkorange", alpha= .7) +
  labs(title="Distribution of user reviews", x = "Number of user reviews") +
  ylab("") +
  theme_minimal()

# critics review histogram
movies %>% ggplot() +
  geom_histogram(aes(x = critics_review), bins = 50, fill = "darkorange", col="darkorange", alpha= .7) +
  labs(title="Distribution of critics reviews", x = "Number of critics reviews") +
  ylab("") +
  theme_minimal()

```


### Possible log transformations
```{r, echo=FALSE, warning = FALSE, message= FALSE, out.width = '50%', fig.height=4}
# log histogram for votes
movies %>% ggplot() +
  geom_histogram(aes(x = log(votes)), fill = "brown1", alpha = 0.7) +
  labs(x = "Ln of number of votes",
       y = "") +
   theme_minimal()

# log histogram for duration
movies %>% ggplot() +
  geom_histogram(aes(x = log(duration)), fill = "brown1",  alpha = 0.7) +
  labs(x = "Ln of movie length",
       y = "") +
  theme_minimal()

# log histogram for gross_income
movies %>% ggplot() +
  geom_histogram(aes(x = log(worldwide_gross_income)), fill = "brown1",  alpha = 0.7) +
  labs(x = "Ln of gross movie income (in millions)",
       y = "") +
   theme_minimal()
# log histogram for user review
movies %>% ggplot() +
  geom_histogram(aes(x = log(user_review)), fill = "brown1",  alpha = 0.7) +
  labs(x = "Ln of user review",
       y = "") +
   theme_minimal()

```

### Transformations for x variables
#### Duration
Log seems better
```{r duration,  echo=FALSE, warning = FALSE, message= FALSE, out.width = '50%', fig.height=4}
chck_sp <- function(x_var){
  ggplot( movies , aes(x = x_var, y = rating)) +
    geom_point() +
    geom_smooth(method="loess" , formula = y ~ x )+
    labs(y = "Weighted average of imdb ratings") +
    theme_minimal()
}
# taking the log first
chck_sp(  movies$duration) 
chck_sp( log( movies$duration) )

```

#### Metacritic
The only variable that shows a linear pattern with IMDB rating thus this is going to be an important variable in the regression model

```{r metacritic , echo=FALSE, warning = FALSE, message= FALSE, out.width = '50%', fig.height=4}
chck_sp( movies$metacritic)
```

#### Gross income

```{r, echo=FALSE, warning = FALSE, message= FALSE, out.width = '50%'}
chck_sp(log (movies$worldwide_gross_income))
```

#### User_review

```{r, echo=FALSE, warning = FALSE, message= FALSE, out.width = '50%'}
chck_sp(log (movies$user_review))
```

#### Critics review
Taking the log does not really help.
```{r, echo=FALSE, warning = FALSE, message= FALSE, out.width = '50%', fig.height=4}
chck_sp( movies$critics_review)
chck_sp( log(movies$critics_review) )
```

### Interaction

The lines between the variables votes - gross income are parallel, so no interaction occures between them, while for ucratio - votes we can see the following graph:

```{r, echo=FALSE, warning = FALSE, message= FALSE, fig.align='center', out.width = '50%', fig.height=4}
intera5 <- lm(rating ~  votes * user_review, data = genre_dist)
plot_model(intera5, type = "pred", terms = c("votes", "user_review"))

```

The plot shows that when user review raises from low to medium there is a slight change (drop) in the output rating. The interaction term is significant at 1%, still these slope differences are not that extreme, almost parallel.






